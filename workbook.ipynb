{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import twint\n",
    "import nest_asyncio\n",
    "from datetime import date\n",
    "\n",
    "def tweet_scrape(search_term):\n",
    "    nest_asyncio.apply()\n",
    "    c = twint.Config()\n",
    "    c.Lang = \"en\"\n",
    "    c.Since = str(date.today())\n",
    "    c.Search = [search_term]\n",
    "    c.Pandas = True\n",
    "    twint.run.Search(c)\n",
    "    Tweets_df = twint.storage.panda.Tweets_df\n",
    "    return Tweets_df\n",
    "\n",
    "\n",
    "Tweets_df = tweet_scrape('hy/droflask')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def vader_scores(tweet):\n",
    "    \"\"\"\n",
    "    Run VADER sentiment analysis and return  values\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return list(vs.values())\n",
    "\n",
    "\n",
    "def score_columns(Tweets_df):\n",
    "    \"\"\"\n",
    "    Apply those values to the tweets_df and then seperate them into distinct columns\n",
    "    \"\"\"\n",
    "    Tweets_df['Sentiment'] = Tweets_df['tweet'].map(vader_scores)\n",
    "    Tweets_df[['negative','neutral','positive', 'compound']] = Tweets_df['Sentiment'].tolist()\n",
    "    Tweets_df.drop(columns='Sentiment', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "def load_bq(dataframe, table):\n",
    "    client = bigquery.Client()\n",
    "    df = dataframe\n",
    "    job = client.load_table_from_dataframe(df, table)\n",
    "    job.result()\n",
    "\n",
    "\n",
    "load_bq(Tweets_df,'deb-01-346001.vibe_shift.hydroflask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datastudio.google.com/s/vAweiTS-KgE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import twint\n",
    "import nest_asyncio\n",
    "from datetime import date\n",
    "\n",
    "def tweet_scrape():\n",
    "    nest_asyncio.apply()\n",
    "    c = twint.Config()\n",
    "    c.Lang = \"en\"\n",
    "    c.Since = '2022-07-21'\n",
    "    c.Username = \"Grimezsz\"\n",
    "    c.Pandas = True\n",
    "    twint.run.Search(c)\n",
    "    Tweets_df = twint.storage.panda.Tweets_df\n",
    "    return grimes_df\n",
    "\n",
    "\n",
    "grimes_df = tweet_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import database, table, search_term\n",
    "import pandas as pd\n",
    "import twint\n",
    "import nest_asyncio\n",
    "from datetime import date\n",
    "\n",
    "def tweet_scrape(search_term):\n",
    "    nest_asyncio.apply()\n",
    "    c = twint.Config()\n",
    "    c.Lang = \"en\"\n",
    "    c.Since = str(date.today())\n",
    "    c.Search = [search_term]\n",
    "    c.Pandas = True\n",
    "    twint.run.Search(c)\n",
    "    Tweets_df = twint.storage.panda.Tweets_df\n",
    "    return Tweets_df\n",
    "\n",
    "heat_df = tweet_scrape(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Expected a string or bytes dtype, got float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jessgschu/projects/vibe-shift/workbook.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=88'>89</a>\u001b[0m heatsample_df \u001b[39m=\u001b[39m heat_df\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=90'>91</a>\u001b[0m \u001b[39m# heatsample_df = heatsample_df.astype(schema)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=91'>92</a>\u001b[0m load_bq(heatsample_df, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdatabase\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mtable\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=93'>94</a>\u001b[0m heatsample_df\u001b[39m.\u001b[39mdtypes\n",
      "\u001b[1;32m/home/jessgschu/projects/vibe-shift/workbook.ipynb Cell 7\u001b[0m in \u001b[0;36mload_bq\u001b[0;34m(dataframe, table)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=5'>6</a>\u001b[0m df \u001b[39m=\u001b[39m dataframe\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=6'>7</a>\u001b[0m job_config \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mLoadJobConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=7'>8</a>\u001b[0m     schema \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m         {\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mSTRING\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=45'>46</a>\u001b[0m         {\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtrans_dest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mSTRING\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=46'>47</a>\u001b[0m ])\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=47'>48</a>\u001b[0m job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mload_table_from_dataframe(df, table, job_config\u001b[39m=\u001b[39;49mjob_config)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jessgschu/projects/vibe-shift/workbook.ipynb#ch0000006vscode-remote?line=48'>49</a>\u001b[0m job\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/google/cloud/bigquery/client.py:2628\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[39mif\u001b[39;00m parquet_compression \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msnappy\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# adjust the default value\u001b[39;00m\n\u001b[1;32m   2626\u001b[0m         parquet_compression \u001b[39m=\u001b[39m parquet_compression\u001b[39m.\u001b[39mupper()\n\u001b[0;32m-> 2628\u001b[0m     _pandas_helpers\u001b[39m.\u001b[39;49mdataframe_to_parquet(\n\u001b[1;32m   2629\u001b[0m         dataframe,\n\u001b[1;32m   2630\u001b[0m         job_config\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m   2631\u001b[0m         tmppath,\n\u001b[1;32m   2632\u001b[0m         parquet_compression\u001b[39m=\u001b[39;49mparquet_compression,\n\u001b[1;32m   2633\u001b[0m         parquet_use_compliant_nested_type\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2634\u001b[0m     )\n\u001b[1;32m   2635\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2636\u001b[0m     dataframe\u001b[39m.\u001b[39mto_parquet(\n\u001b[1;32m   2637\u001b[0m         tmppath,\n\u001b[1;32m   2638\u001b[0m         engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2644\u001b[0m         ),\n\u001b[1;32m   2645\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/google/cloud/bigquery/_pandas_helpers.py:672\u001b[0m, in \u001b[0;36mdataframe_to_parquet\u001b[0;34m(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwargs \u001b[39m=\u001b[39m (\n\u001b[1;32m    666\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39muse_compliant_nested_type\u001b[39m\u001b[39m\"\u001b[39m: parquet_use_compliant_nested_type}\n\u001b[1;32m    667\u001b[0m     \u001b[39mif\u001b[39;00m _helpers\u001b[39m.\u001b[39mPYARROW_VERSIONS\u001b[39m.\u001b[39muse_compliant_nested_type\n\u001b[1;32m    668\u001b[0m     \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    669\u001b[0m )\n\u001b[1;32m    671\u001b[0m bq_schema \u001b[39m=\u001b[39m schema\u001b[39m.\u001b[39m_to_schema_fields(bq_schema)\n\u001b[0;32m--> 672\u001b[0m arrow_table \u001b[39m=\u001b[39m dataframe_to_arrow(dataframe, bq_schema)\n\u001b[1;32m    673\u001b[0m pyarrow\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_table(\n\u001b[1;32m    674\u001b[0m     arrow_table,\n\u001b[1;32m    675\u001b[0m     filepath,\n\u001b[1;32m    676\u001b[0m     compression\u001b[39m=\u001b[39mparquet_compression,\n\u001b[1;32m    677\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    678\u001b[0m )\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/google/cloud/bigquery/_pandas_helpers.py:617\u001b[0m, in \u001b[0;36mdataframe_to_arrow\u001b[0;34m(dataframe, bq_schema)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39mfor\u001b[39;00m bq_field \u001b[39min\u001b[39;00m bq_schema:\n\u001b[1;32m    615\u001b[0m     arrow_names\u001b[39m.\u001b[39mappend(bq_field\u001b[39m.\u001b[39mname)\n\u001b[1;32m    616\u001b[0m     arrow_arrays\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 617\u001b[0m         bq_to_arrow_array(get_column_or_index(dataframe, bq_field\u001b[39m.\u001b[39;49mname), bq_field)\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m     arrow_fields\u001b[39m.\u001b[39mappend(bq_to_arrow_field(bq_field, arrow_arrays[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtype))\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m((field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m arrow_fields)):\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/google/cloud/bigquery/_pandas_helpers.py:342\u001b[0m, in \u001b[0;36mbq_to_arrow_array\u001b[0;34m(series, bq_field)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m field_type_upper \u001b[39min\u001b[39;00m schema\u001b[39m.\u001b[39m_STRUCT_TYPES:\n\u001b[1;32m    341\u001b[0m     \u001b[39mreturn\u001b[39;00m pyarrow\u001b[39m.\u001b[39mStructArray\u001b[39m.\u001b[39mfrom_pandas(series, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39marrow_type)\n\u001b[0;32m--> 342\u001b[0m \u001b[39mreturn\u001b[39;00m pyarrow\u001b[39m.\u001b[39;49mArray\u001b[39m.\u001b[39;49mfrom_pandas(series, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49marrow_type)\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/pyarrow/array.pxi:1033\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/pyarrow/array.pxi:312\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/pyarrow/array.pxi:83\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/vibe-shift/venv/lib/python3.8/site-packages/pyarrow/error.pxi:123\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: Expected a string or bytes dtype, got float64"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "def load_bq(dataframe, table):\n",
    "    client = bigquery.Client()\n",
    "    df = dataframe\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema = [\n",
    "            {'name': 'id', 'type': 'STRING'},\n",
    "            {'name': 'conversation_id', 'type': 'STRING'},\n",
    "            {'name': 'created_at', 'type': 'STRING'},\n",
    "            {'name': 'date', 'type': 'DATETIME'},\n",
    "            {'name': 'timezone', 'type': 'STRING'},\n",
    "            {'name': 'place', 'type': 'STRING'},\n",
    "            {'name': 'tweet', 'type': 'STRING'},\n",
    "            {'name': 'language', 'type': 'STRING'},\n",
    "            {'name': 'hashtags', 'type': 'ARRAY'},\n",
    "            {'name': 'cashtags', 'type': 'ARRAY'},\n",
    "            {'name': 'user_id', 'type': 'STRING'},\n",
    "            {'name': 'user_id_str', 'type': 'STRING'},\n",
    "            {'name': 'username', 'type': 'STRING'},\n",
    "            {'name': 'name', 'type': 'STRING'},\n",
    "            {'name': 'day', 'type': 'INT64'},\n",
    "            {'name': 'hour', 'type': 'INT64'},\n",
    "            {'name': 'link', 'type': 'STRING'},\n",
    "            {'name': 'urls', 'type': 'STRING'},\n",
    "            {'name': 'photos', 'type': 'STRING'},\n",
    "            {'name': 'video', 'type': 'STRING'},\n",
    "            {'name': 'thumbnail', 'type': 'STRING'},\n",
    "            {'name': 'retweet', 'type': 'BOOL'},\n",
    "            {'name': 'nlikes', 'type': 'INT64'},\n",
    "            {'name': 'nreplies', 'type': 'INT64'},\n",
    "            {'name': 'nretweets', 'type': 'INT64'},\n",
    "            {'name': 'quote_url', 'type': 'STRING'},\n",
    "            {'name': 'search', 'type': 'STRING'},\n",
    "            {'name': 'near', 'type': 'STRING'},\n",
    "            {'name': 'geo', 'type': 'STRING'},\n",
    "            {'name': 'source', 'type': 'STRING'},\n",
    "            {'name': 'user_rt_id', 'type': 'STRING'},\n",
    "            {'name': 'user_rt', 'type': 'STRING'},\n",
    "            {'name': 'retweet_id', 'type': 'STRING'},\n",
    "            {'name': 'reply_to', 'type': 'STRING'},\n",
    "            {'name': 'retweet_date', 'type': 'STRING'},\n",
    "            {'name': 'translate', 'type': 'STRING'},\n",
    "            {'name': 'trans_src', 'type': 'STRING'},\n",
    "            {'name': 'trans_dest', 'type': 'STRING'}\n",
    "    ])\n",
    "    job = client.load_table_from_dataframe(df, table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "schema = {'id': str, \n",
    "            'conversation_id': str, \n",
    "            'created_at': str,\n",
    "            'date': str,\n",
    "            'timezone': str,\n",
    "            'place': str,\n",
    "            'tweet': str,\n",
    "            'language': str,\n",
    "            'hashtags': object,\n",
    "            'cashtags': object,\n",
    "            'user_id': str,\n",
    "            'user_id_str': str,\n",
    "            'username': str,\n",
    "            'name': str,\n",
    "            'day': int,\n",
    "            'hour': int,\n",
    "            'link': str,\n",
    "            'urls': str,\n",
    "            'photos': str,\n",
    "            'video': str,\n",
    "            'thumbnail': str,\n",
    "            'retweet': bool,\n",
    "            'nlikes': int,\n",
    "            'nreplies': int,\n",
    "            'nretweets': int,\n",
    "            'quote_url': str,\n",
    "            'search': str,\n",
    "            'near': str,\n",
    "            'geo': str,\n",
    "            'source': str,\n",
    "            'user_rt_id': str,\n",
    "            'user_rt': str,\n",
    "            'retweet_id': str,\n",
    "            'reply_to': str,\n",
    "            'retweet_date': str,\n",
    "            'translate': str,\n",
    "            'trans_src': str,\n",
    "            'trans_dest': str}\n",
    "            \n",
    "heatsample_df = heat_df.sample(frac=0.25)\n",
    "\n",
    "heatsample_df = heatsample_df.astype(schema)\n",
    "load_bq(heatsample_df, f'{database}{table}')\n",
    "\n",
    "heatsample_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'0': 1550269353194672130, '1': 15502693476749...</td>\n",
       "      <td>{'0': 1658447991000, '1': 1658447989000, '2': ...</td>\n",
       "      <td>{'0': 'his little bye 😭☹️ ive missed him sm gl...</td>\n",
       "      <td>{'0': 'en', '1': 'en', '2': 'en', '3': 'en', '...</td>\n",
       "      <td>{'0': 'https://twitter.com/wcstsidesugg/status...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  {'0': 1550269353194672130, '1': 15502693476749...   \n",
       "\n",
       "                                                date  \\\n",
       "0  {'0': 1658447991000, '1': 1658447989000, '2': ...   \n",
       "\n",
       "                                             content  \\\n",
       "0  {'0': 'his little bye 😭☹️ ive missed him sm gl...   \n",
       "\n",
       "                                                lang  \\\n",
       "0  {'0': 'en', '1': 'en', '2': 'en', '3': 'en', '...   \n",
       "\n",
       "                                                 url  \n",
       "0  {'0': 'https://twitter.com/wcstsidesugg/status...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snscrape\n",
    "import pandas as pd\n",
    "\n",
    "dune_df = pd.read_json('little_miss_sentiment.json', lines=True)\n",
    "dune_df = dune_df[['id', 'date', 'content', 'lang', 'url']]\n",
    "\n",
    "\n",
    "dune_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-07-28'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternate GBQ\n",
    "    # client = bigquery.Client()\n",
    "    # sent_df = pd.read_json(f'/opt/airflow/dags/data/{table}_sentiment.json', lines=True)\n",
    "    # schema = [{'name':'id', 'type': 'STRING'},\n",
    "    #     {'name':'date', 'type': 'DATETIME'},\n",
    "    #     {'name': 'content', 'type': 'STRING'},\n",
    "    #     {'name': 'lang', 'type': 'STRING'},\n",
    "    #     {'name': 'url', 'type': 'STRING'},\n",
    "    #     {'name': 'negative', 'type': 'FLOAT'},\n",
    "    #     {'name': 'neutral', 'type': 'FLOAT'},\n",
    "    #     {'name': 'positive', 'type': 'FLOAT'},\n",
    "    #     {'name': 'copmpund', 'type': 'FLOAT'}]\n",
    "    # sent_df.to_gbq(f'{dataset}.{table}', project_id=f'{project_id}', if_exists='append', table_schema=schema)\n",
    "\n",
    "# def to_parquet(Tweets_df):\n",
    "#     Tweets_df.to_parquet(f'/opt/airflow/dags/data/{table}.parquet')\n",
    "\n",
    "# def load_bq(dataframe, table):\n",
    "#     client = bigquery.Client()\n",
    "#     # os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/google_creds/goog_creds.json\"\n",
    "#     df = dataframe\n",
    "#     job = client.load_table_from_dataframe(df, table)\n",
    "#     job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-07-27'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "str(date.today() - timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROPOSAL\n",
    "\n",
    "## Vibe Shift\n",
    "### *A Twitter Sentiment Analysis*\n",
    "\n",
    "A pipeline that uses snscrape to scrape Twitter for tweets on a particular subject matter, pandas to do basic cleaning, VADER sentiment analysis to show public opinion towards the subject, stores the data in a bigquery dataset. Can run reports using Google Data Studio to show changing sentiments over time, automated with Airflow. \n",
    "\n",
    "MVP - \n",
    "\n",
    "* Tweets scraped, cleaned and stored\n",
    "* At least one trackable word, name, or phrase\n",
    "* Column in data with basic sentiment analysis (positive, neutral, nagative)\n",
    "* Database on gbq\n",
    "* Graph showing updated sentiment\n",
    "* Automate with Airflow\n",
    "\n",
    "Stretch Goals - \n",
    "* Ability to simultaneously moniter more than one word, name, or phrase\n",
    "* More advanced sentiment analysis\n",
    "* Region specific sentiment analysis\n",
    "* Translation of tweets in other languages (???)\n",
    "* Google trend comparison, how much are people interested in it vs sentiment and are these related\n",
    "\n",
    "Data Studio \n",
    "!['Graph illustrating trend in sentiment'](img/stardew27.png)\n",
    "\n",
    "!['Graph illustrating trend in sentiment'](img/littlemiss.png)\n",
    "\n",
    "Tools - \n",
    "* Python\n",
    "* Apache Airflow\n",
    "* [SNScrape](https://github.com/JustAnotherArchivist/snscrape)\n",
    "* [VADER Sentiment Analysis](https://github.com/cjhutto/vaderSentiment) \n",
    "* Google BigQuery\n",
    "* Google Data Studio\n",
    "\n",
    "Repository - \n",
    "\n",
    "https://github.com/jessgschueler/vibe-shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usage: snscrape [-h] [--version] [--citation] [-v] [--dump-locals] [--retry N] [-n N] [-f FORMAT | --jsonl] [--with-entity]\n",
    "                [--since DATETIME] [--progress]\n",
    "                SCRAPER ...\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --version             show program's version number and exit\n",
    "  --citation            Display recommended citation information and exit (default: None)\n",
    "  -v, --verbose, --verbosity\n",
    "                        Increase output verbosity (default: 0)\n",
    "  --dump-locals         Dump local variables on serious log messages (warnings or higher) (default: False)\n",
    "  --retry N, --retries N\n",
    "                        When the connection fails or the server returns an unexpected response, retry up to N times with an\n",
    "                        exponential backoff (default: 3)\n",
    "  -n N, --max-results N\n",
    "                        Only return the first N results (default: None)\n",
    "  -f FORMAT, --format FORMAT\n",
    "                        Output format (default: None)\n",
    "  --jsonl               Output JSONL (default: False)\n",
    "  --with-entity         Include the entity (e.g. user, channel) as the first output item (default: False)\n",
    "  --since DATETIME      Only return results newer than DATETIME (default: None)\n",
    "  --progress            Report progress on stderr (default: False)\n",
    "\n",
    "scrapers:\n",
    "  SCRAPER\n",
    "    facebook-community\n",
    "    facebook-group\n",
    "    facebook-user\n",
    "    instagram-hashtag\n",
    "    instagram-location\n",
    "    instagram-user\n",
    "    mastodon-profile\n",
    "    mastodon-toot\n",
    "    reddit-search\n",
    "    reddit-subreddit\n",
    "    reddit-user\n",
    "    telegram-channel\n",
    "    twitter-hashtag\n",
    "    twitter-list-posts\n",
    "    twitter-profile\n",
    "    twitter-search\n",
    "    twitter-trends\n",
    "    twitter-tweet\n",
    "    twitter-user\n",
    "    vkontakte-user\n",
    "    weibo-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_ii58jep</td>\n",
       "      <td>2022-07-29T16:23:04+00:00</td>\n",
       "      <td>i change my mind every few days. i swap betwee...</td>\n",
       "      <td>https://old.reddit.com/r/kindle/comments/wb2b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t1_ihynz1l</td>\n",
       "      <td>2022-07-28T07:24:59+00:00</td>\n",
       "      <td>You should try emailing baggu and asking if th...</td>\n",
       "      <td>https://old.reddit.com/r/BAGGU/comments/wa1gh1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_wa1gh1</td>\n",
       "      <td>2022-07-28T05:48:09+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/BAGGU/comments/wa1gh1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_w9rm6u</td>\n",
       "      <td>2022-07-27T22:11:34+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/BAGGU/comments/w9rm6u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_ihw9f76</td>\n",
       "      <td>2022-07-27T19:55:45+00:00</td>\n",
       "      <td>That baggu bag looks cool, but cannot get over...</td>\n",
       "      <td>https://old.reddit.com/r/onebag/comments/w9hfn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t1_ihvoxoz</td>\n",
       "      <td>2022-07-27T17:47:40+00:00</td>\n",
       "      <td>I’m from Hawai’i and beach towels are my life,...</td>\n",
       "      <td>https://old.reddit.com/r/blogsnark/comments/w9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_w9hfn3</td>\n",
       "      <td>2022-07-27T15:17:49+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/onebag/comments/w9hfn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t1_iht2mef</td>\n",
       "      <td>2022-07-27T03:33:32+00:00</td>\n",
       "      <td>You might like the Baggu medium nylon crescent...</td>\n",
       "      <td>https://old.reddit.com/r/HerOneBag/comments/w8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3_w91r9f</td>\n",
       "      <td>2022-07-27T01:53:04+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/DealPersonal/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3_w91ewz</td>\n",
       "      <td>2022-07-27T01:37:01+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://old.reddit.com/r/DealPersonal/comments...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                    created  \\\n",
       "0  t1_ii58jep  2022-07-29T16:23:04+00:00   \n",
       "1  t1_ihynz1l  2022-07-28T07:24:59+00:00   \n",
       "2   t3_wa1gh1  2022-07-28T05:48:09+00:00   \n",
       "3   t3_w9rm6u  2022-07-27T22:11:34+00:00   \n",
       "4  t1_ihw9f76  2022-07-27T19:55:45+00:00   \n",
       "5  t1_ihvoxoz  2022-07-27T17:47:40+00:00   \n",
       "6   t3_w9hfn3  2022-07-27T15:17:49+00:00   \n",
       "7  t1_iht2mef  2022-07-27T03:33:32+00:00   \n",
       "8   t3_w91r9f  2022-07-27T01:53:04+00:00   \n",
       "9   t3_w91ewz  2022-07-27T01:37:01+00:00   \n",
       "\n",
       "                                                body  \\\n",
       "0  i change my mind every few days. i swap betwee...   \n",
       "1  You should try emailing baggu and asking if th...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  That baggu bag looks cool, but cannot get over...   \n",
       "5  I’m from Hawai’i and beach towels are my life,...   \n",
       "6                                                NaN   \n",
       "7  You might like the Baggu medium nylon crescent...   \n",
       "8                                                NaN   \n",
       "9                                                NaN   \n",
       "\n",
       "                                                 url  \n",
       "0  https://old.reddit.com/r/kindle/comments/wb2b7...  \n",
       "1  https://old.reddit.com/r/BAGGU/comments/wa1gh1...  \n",
       "2  https://old.reddit.com/r/BAGGU/comments/wa1gh1...  \n",
       "3  https://old.reddit.com/r/BAGGU/comments/w9rm6u...  \n",
       "4  https://old.reddit.com/r/onebag/comments/w9hfn...  \n",
       "5  https://old.reddit.com/r/blogsnark/comments/w9...  \n",
       "6  https://old.reddit.com/r/onebag/comments/w9hfn...  \n",
       "7  https://old.reddit.com/r/HerOneBag/comments/w8...  \n",
       "8  https://old.reddit.com/r/DealPersonal/comments...  \n",
       "9  https://old.reddit.com/r/DealPersonal/comments...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reddit():\n",
    "    reddit_df = pd.read_json('baggu.json', lines=True)\n",
    "    reddit_df = reddit_df[['id', 'created', 'body', 'url']]\n",
    "    return reddit_df\n",
    "\n",
    "reddit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "648b5abb34e66cab5d7445b3f5c17cae57dc0cdf7836e4133da0f21e9cafd581"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
